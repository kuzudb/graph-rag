{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph RAG\n",
    "\n",
    "\"Graph RAG\" has become an incredible buzz term of late. The goal of this notebook is to provide a simple and intuitive demonstration of what Graph RAG accomplishes, and how to use two open source, embedded databases (Kùzu, for graph traversal, and LanceDB, for vector search) to combine the benefits of vector and graph databases.\n",
    "\n",
    "## What is Graph RAG?\n",
    "\n",
    "At its core, Graph RAG aims to combine the power of knowledge graphs with the well-known benefits of vector (semantic) search. Knowledge graphs are great for storing and traversing relationships between entities, while vector embeddings are great for capturing the semantic similarity between chunks of data. By combining the two, we can create a powerful system that can answer complex queries that require both semantic similarity and relationship traversal.\n",
    "\n",
    "## How and why does Graph RAG work in practice?\n",
    "\n",
    "It's worth going over how and why Graph RAG makes sense, intuitively. Semantic search based on vector similarity leverages the _implicit_ relationships between entities - two vector embeddings that represent different chunks of text may be close to each other in vector space, indicating that they are semantically similar. On the other hand, knowledge graphs store _explicit_ relationships between entities - two nodes in a graph may be connected by an explicit relationship (termed an \"edge\"), indicating that they are related.\n",
    "\n",
    "By combining the two, we can create a system that can answer complex queries that require both semantic similarity and relationship traversal. The code in this notebook demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "COHERE_API_KEY = os.environ.get(\"COHERE_API_KEY\")\n",
    "\n",
    "assert OPENAI_API_KEY is not None, \"OPENAI_API_KEY is not set\"\n",
    "assert COHERE_API_KEY is not None, \"COHERE_API_KEY is not set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Set up the embedding model and LLM\n",
    "embed_model = OpenAIEmbedding(model_name=\"text-embedding-3-small\")\n",
    "extraction_llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "generation_llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
    "\n",
    "# Load the dataset on Marie Curie\n",
    "documents = SimpleDirectoryReader(\"../../data/curie/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Vector-only retrieval\n",
    "\n",
    "This stage demonstrates how to extract information into a vector database and store it in [LanceDB](https://lancedb.com/), an open source, embedded vector database. The aim of this stage is to use the vector database to answer the questions using vector-only retrieval, commonly known as \"naive RAG\" or traditional RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prrao/code/graph-rag/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# We'll use LanceDB to perform vector similarity search\n",
    "shutil.rmtree(\"./test_lancedb\", ignore_errors=True)\n",
    "\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-28T22:55:33Z WARN  lance::dataset] No existing dataset at /Users/prrao/code/graph-rag/src/test_lancedb/vectors.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "vector_store = LanceDBVectorStore(\n",
    "    uri=\"./test_lancedb\",\n",
    "    mode=\"overwrite\",\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "    llm=OpenAI(model=\"gpt-4o-mini\", temperature=0.3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre Curie discovered Piezoelectricity.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_retriever = vector_index.as_retriever(similarity_top_k=4)\n",
    "vector_query_engine = RetrieverQueryEngine(vector_retriever)\n",
    "\n",
    "response = vector_query_engine.query(\"Who discovered Piezoelectricty?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre Curie worked with his brother Jacques in discovering piezoelectricity.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = vector_query_engine.query(\"Who did Pierre Curie work with?\")\n",
    "str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implicit relationship \"was a student of\" isn't close enough to \"worked with\" in vector space. This leads the vector search to miss the relationship between Pierre Curie and Paul Langevin (who was his student, meaning that they worked togethrer). Using the graph as shown earlier, we were able to explicitly define and capture this relationship, allowing the graph-based retrieval to provide the generation model with a slightly better context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from vector-only retrieval\n",
    "\n",
    "Due to the nature of the data and the questions being asked, the vector-only retrieval obtains _partial_ answers to the question \"Who did Pierre Curie work with?\". This is because the vector embeddings are not able to capture the deeper relationships between the entities in the text. This is where the graph-based retrieval provides value, as it can capture these relationships and provide more accurate answers to certain questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Graph-only retrieval\n",
    "\n",
    "Next, let's demonstrate how to extract information into a knowledge graph and store it in [Kùzu](https://kuzudb.com/), an open source, embedded graph database.\n",
    "The aim of this stage is to use the graph to answer the same questions as in Part 1, but using graph-only retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import List, Literal, Optional\n",
    "from llama_index.core import PropertyGraphIndex, Settings\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.graph_stores.kuzu import KuzuPropertyGraphStore\n",
    "\n",
    "import kuzu\n",
    "\n",
    "shutil.rmtree(\"test_kuzudb\", ignore_errors=True)\n",
    "db = kuzu.Database(\"test_kuzudb\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the allowed entities and relationships\n",
    "entities = Literal[\"PERSON\", \"NOBEL_PRIZE\", \"LOCATION\", \"DISCOVERY\"]\n",
    "relations = Literal[\"DISCOVERED\", \"IS_MARRIED_TO\", \"WORKED_WITH\", \"WON\"]\n",
    "\n",
    "# Define explicit relationship directions as a list of triples\n",
    "# The graph extraction process will be guided by this schema\n",
    "validation_schema = [\n",
    "    (\"PERSON\", \"IS_MARRIED_TO\", \"PERSON\"),\n",
    "    (\"PERSON\", \"WORKED_WITH\", \"PERSON\"),\n",
    "    (\"PERSON\", \"WON\", \"NOBEL_PRIZE\"),\n",
    "    (\"PERSON\", \"DISCOVERED\", \"DISCOVERY\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_store = KuzuPropertyGraphStore(\n",
    "    db,\n",
    "    has_structured_schema=True,\n",
    "    relationship_schema=validation_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_path_extractor = SchemaLLMPathExtractor(\n",
    "    llm=extraction_llm,\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=validation_schema,\n",
    "    strict=True,  # if false, will allow triples outside of the schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 704.21it/s]\n",
      "Extracting paths from text with schema: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Set up the property graph index\n",
    "kg_index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=embed_model,\n",
    "    kg_extractors=[schema_path_extractor],\n",
    "    property_graph_store=graph_store,\n",
    "    show_progress=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is created, we can explore it in [Kùzu Explorer](https://github.com/kuzudb/explorer), a web-base UI, by running a Docker container that pulls the latest image of Kùzu Explorer as follows:\n",
    "\n",
    "```bash\n",
    "docker run -p 8000:8000 \\\n",
    "           -v ./test_kuzudb:/database \\\n",
    "           -e MODE=READ_ONLY \\\n",
    "           --rm kuzudb/explorer:latest\n",
    "```\n",
    "\n",
    "Then, launch the UI and then visting http://localhost:8000/.\n",
    "\n",
    "The easiest way to see the entire graph is to use a Cypher query like `MATCH (a)-[b]->(c) RETURN * LIMIT 100`.\n",
    "\n",
    "For this dataset, the graph constructed looks as follows:\n",
    "\n",
    "![](../../assets/curie.png)\n",
    "\n",
    "The dataset is about the scientist Marie Curie and her discoveries, as well as her direct and indirect relationships to persons like Pierre Curie, Paul Langevin and Albert Einstein. The graph has an explicit schema, specified by us, and captures entities from the unstructured data like \"Polonium\", \"Radium\", and \"Nobel Prize in Physics\", etc., and edges representing relationships between these entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of graph quality\n",
    "\n",
    "Graph construction is a critical step in the process of building a Graph RAG system. The quality of the graph will directly impact the quality of the results. In this notebook, we will use a simple example that leverages an LLM to demonstrate the idea. In practice, you would use more sophisticated methods to construct a knowledge graph, such as custom ML models or APIs (Rebel, GliNER/GliREL, DiffBot, WhyHow Knowledge Graph Studio, etc.).\n",
    "\n",
    "The key is to _persist_ the graph in a graph database, so that it can be managed and queried efficiently. Kùzu is a great choice for this purpose, as it is an open source, embedded graph database that is easy to use and deploy.\n",
    "\n",
    "The LLM-generated graph can be incomplete, noisy, or contain errors. It is important to clean and refine the graph before storing it in the database. This process is called \"graph curation\" and is essential for the quality of the results. The following cell performs the task of explicitly defining the graph and storing specific nodes and relationships in the already-existing knowledge graph, and persists it to the Kùzu database that sits on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.graph_stores.types import Relation, EntityNode\n",
    "\n",
    "graph_store.upsert_nodes(\n",
    "    [\n",
    "        EntityNode(label=\"PERSON\", name=\"Jacques Curie\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "graph_store.upsert_relations(\n",
    "    [\n",
    "        Relation(\n",
    "            label=\"WORKED_WITH\",\n",
    "            source_id=\"Pierre Curie\",\n",
    "            target_id=\"Paul Langevin\",\n",
    "        ),\n",
    "        Relation(\n",
    "            label=\"DISCOVERED\",\n",
    "            source_id=\"Jacques Curie\",\n",
    "            target_id=\"piezoelectricity\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Curie and Jacques Curie discovered piezoelectricity.\n"
     ]
    }
   ],
   "source": [
    "kg_retriever = kg_index.as_retriever()\n",
    "kg_query_engine = kg_index.as_query_engine(include_text=False)\n",
    "\n",
    "response = kg_query_engine.query(\"Who discovered Piezoelectricity?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Curie worked with Jacques Curie and Paul Langevin.\n"
     ]
    }
   ],
   "source": [
    "response = kg_query_engine.query(\"Who did Pierre Curie work with?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two explicit relationships we are interested in are:\n",
    "- Pierre Curie worked with his brother Jacques, to discover piezoelectricity.\n",
    "- Paul Langevin was a student of Pierre Curie, which can be interpreted the same as a \"worked with\" relationship.\n",
    "\n",
    "Explicitly modeling this and storing this in the graph allowed the information to be retrieved, providing the right context to the generation model downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from graph-only retrieval\n",
    "\n",
    "It can be seen by inspecting the raw data that the LLM-extracted graph is incomplete. Once the right nodes/relationships are added to the graph, the quality of the graph-based retrieval improves significantly. This did require some manual curation, but we will demonstrate below that this process is worth it, by trying to answer the **same** questions using vector-only retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: Combining graph and vector retrieval to build a Graph RAG system\n",
    "\n",
    "In this stage, we will demonstrate how to combine graph and vector retrieval and rerank the results in order to provide better context to the LLM prior to generating the response. We will use the afore-mentioned Kùzu and LanceDB databases to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.postprocessor.cohere_rerank.base import CohereRerank\n",
    "\n",
    "\n",
    "class CustomRerankerRetriever(BaseRetriever):\n",
    "    \"\"\"Custom retriever with cohere reranking.\"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            kg_retriever,\n",
    "            vector_retriever,\n",
    "            cohere_api_key: Optional[str] = None,\n",
    "            cohere_top_n: int = 2,\n",
    "        ):\n",
    "        self._kg_retriever = kg_retriever\n",
    "        self._vector_retriever = vector_retriever\n",
    "        self._reranker = CohereRerank(\n",
    "            api_key=cohere_api_key, top_n=cohere_top_n\n",
    "        )\n",
    "\n",
    "    def _retrieve(self, query: str) -> List[NodeWithScore]:\n",
    "        \"\"\"Define custom retriever with reranking.\n",
    "\n",
    "        Could return `str`, `TextNode`, `NodeWithScore`, or a list of those.\n",
    "        \"\"\"\n",
    "        vector_retrieval_nodes = self._vector_retriever.retrieve(query)\n",
    "        kg_retrieval_nodes = self._kg_retriever.retrieve(query)\n",
    "        combined_nodes = vector_retrieval_nodes + kg_retrieval_nodes\n",
    "        reranked_nodes = self._reranker.postprocess_nodes(\n",
    "            combined_nodes,\n",
    "            query_str=str(query),\n",
    "        )\n",
    "        unique_nodes = {n.node_id: n for n in reranked_nodes}\n",
    "        return list(unique_nodes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_reranker_retriever = CustomRerankerRetriever(\n",
    "    kg_retriever,\n",
    "    vector_retriever,\n",
    "    cohere_api_key=COHERE_API_KEY,\n",
    "    cohere_top_n=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Curie worked with Paul Langevin and Jacques Curie.\n"
     ]
    }
   ],
   "source": [
    "# Set the LLM for generation in the CustomRerankerRetriever\n",
    "Settings.llm = generation_llm\n",
    "\n",
    "custom_reranker_query_engine = RetrieverQueryEngine(custom_reranker_retriever)\n",
    "\n",
    "response = custom_reranker_query_engine.query(\"Who did Pierre Curie work with?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom retriever was able to use the context from both the graph and the vector retrievals to provide the correct answer to the question - Paul Langevin was Pierre Curie's student as per the raw text, but the knowledge graph explicitly stored this via the relationship `(:PERSON {name: \"Paul Langevin\"})-[:WORKED_WITH]->(:PERSON {name: \"PierreCurie\"})`, which the reranker retriever was able to leverage from the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "As can be seen from this demo, combining graph and vector retrieval can, on average, provide more accurate and contextually relevant answers to the questions. In certain cases, the vector retrieval can retrieve answers that are relevant through the fuzzy relationships that are implicitly modelled via the embeddings. In other cases, the graph retrieval can provide more relevant, complete answers, because it *explicitly* models the relationships between the entities in the text. It's important to note that the two types of retrievals can complement each other, rather than being in competition.\n",
    "\n",
    "By combining vector and graph retrieval and a reranker retriever that can leverage the context from both kinds of retrieval to provide the generation LLM with more relevant and accurate context, and thus a more relevant answer than if either retrieval method was used on its own.\n",
    "\n",
    "In practice, the Graph RAG system can be used to answer a wide range of questions, such as factual questions, definition questions, and reasoning questions. The key is to build a high-quality knowledge graph, and to combine it with vector search in a way that provides the most relevant and accurate answers to the questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
